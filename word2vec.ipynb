{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Embedding for Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vips/share/Vu\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "Tokenizer = Okt()\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "path = os.path.dirname(os.getcwd())\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_remove = [\"Eomi\", \"Punctuation\", \"Unknown\", \"KoreanParticle\", \"Hashtag\", \"ScreenName\",\n",
    "#                     \"Email\", \"URL\", \"Josa\", \"PreEomi\", \"Foreign\"]\n",
    "pos_remove = [\"Eomi\", \"Punctuation\", \"Unknown\", \"KoreanParticle\", \"Hashtag\", \"ScreenName\",\n",
    "                    \"Email\", \"URL\", \"PreEomi\", \"Foreign\"]\n",
    "\n",
    "def simple_process_tokens(sent):\n",
    "    tokens = Tokenizer.pos(sent, stem=True, norm=True)\n",
    "    out = []\n",
    "    for (token, type) in tokens:\n",
    "        if(type in pos_remove): \n",
    "            continue\n",
    "        else:\n",
    "            out.append(token)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ko = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800404\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"kornli\", \"multinli.train.ko.tsv\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += line.split(\"\\t\")[:-1]\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"kornli\", \"xnli.dev.ko.tsv\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += line.split(\"\\t\")[:-1]\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"kornli\", \"xnli.test.ko.tsv\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += line.split(\"\\t\")[:-1]\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "texts_ko.update({\"kornli\": [simple_process_tokens(doc) for doc in sentences]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17256\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"korsts\", \"sts-dev.tsv\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += line.split(\"\\t\")[-2:]\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"korsts\", \"sts-test.tsv\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += line.split(\"\\t\")[-2:]\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"korsts\", \"sts-train.tsv\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += line.split(\"\\t\")[-2:]\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "texts_ko.update({\"korsts\": [simple_process_tokens(doc) for doc in sentences]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"nsmc\", \"ratings_train.txt\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += [line.split(\"\\t\")[1]]\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"nsmc\", \"ratings_test.txt\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sentences += [line.split(\"\\t\")[1]]\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "# texts_ko += [process_tokens(doc) for doc in sentences]\n",
    "texts_ko.update({\"nsmc\": [simple_process_tokens(doc) for doc in sentences]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"naver-ner\", \"train.tsv\"), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        sentences += [line.split(\"\\t\")[0]]\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"naver-ner\", \"test.tsv\"), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        sentences += [line.split(\"\\t\")[0]]\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "\n",
    "texts_ko.update({\"ner\": [simple_process_tokens(doc) for doc in sentences]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(texts_ko))\n",
    "with open('NLP_data/Tokens_dict.json', 'w', encoding= 'utf-8') as fp:\n",
    "    json.dump(texts_ko, fp,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"NLP_data\", \"stop_words.json\"), 'r') as f:\n",
    "    stop_words = json.load(f)\n",
    "\n",
    "def stop_words_remove(sent):\n",
    "    new_tokens = []\n",
    "    for token in sent:\n",
    "        if(token in stop_words):\n",
    "            continue\n",
    "        new_tokens.append(token)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsmc\n",
      "kornli\n",
      "korsts\n",
      "ner\n"
     ]
    }
   ],
   "source": [
    "Tokens_dict = {\n",
    "    \"nsmc\": [],\n",
    "    \"kornli\": [],\n",
    "    \"korsts\": [],\n",
    "    \"ner\": []\n",
    "}\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"Tokens_dict.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for key in Tokens_dict:\n",
    "        print(key)\n",
    "        for sent in data[key]:\n",
    "            Tokens_dict[key].append(stop_words_remove(sent))\n",
    "\n",
    "with open('NLP_data/Tokens_dict_remove_stop_words.json', 'w', encoding= 'utf-8') as fp:\n",
    "    json.dump(Tokens_dict, fp,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"NLP_data\", \"Tokens_dict_remove_stop_words.json\"), 'r') as f:\n",
    "    data = json.load(f)\n",
    "    nscm_data = data[\"nsmc\"]\n",
    "\n",
    "\n",
    "sentences = []\n",
    "i = 0\n",
    "with open(os.path.join(\"NLP_data\", \"nsmc\", \"ratings_train.txt\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sent = line.split(\"\\t\")\n",
    "        sentences += [(sent[1],  nscm_data[i], int(sent[2]))]\n",
    "        i += 1\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"nsmc_train.json\"), 'w') as fp:\n",
    "    json.dump(sentences, fp,ensure_ascii=False)\n",
    "\n",
    "sentences = []\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"nsmc\", \"ratings_test.txt\"), 'r') as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        sent = line.split(\"\\t\")\n",
    "        sentences += [(sent[1],  nscm_data[i], int(sent[2]))]\n",
    "        i += 1\n",
    "\n",
    "with open(os.path.join(\"NLP_data\", \"nsmc_test.json\"), 'w') as fp:\n",
    "    json.dump(sentences, fp,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107660\n"
     ]
    }
   ],
   "source": [
    "concat_texts_ko = Tokens_dict[\"kornli\"] + Tokens_dict[\"korsts\"] + Tokens_dict[\"ner\"] + Tokens_dict[\"nsmc\"]\n",
    "print(len(concat_texts_ko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=concat_texts_ko, vector_size=128, window=5, min_count=5, workers=4)\n",
    "model.save(\"ko_word2vec.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('GNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e215ceb6bc66c02176aac9a16ef7bdd77eb4405484a4a652fefcacf52fac4f29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
