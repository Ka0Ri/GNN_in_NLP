{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Graph Neural Network for NLP task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "Tokenizer = Okt()\n",
    "\n",
    "path = os.path.dirname(os.getcwd())\n",
    "print(path)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = {\n",
    "\n",
    "## model [\"GNN\", \"RNN\", \"LSTM\", \"BERT\", \"FAST\"]\n",
    "    \"NAME\": \"GNN\",\n",
    "    \n",
    "## Path params\n",
    "    \"DATA\": os.path.join(\"NLP_data\", \"nsmc_test.json\"),\n",
    "    \"MODEL_NAME\": \"ko_word2vec.model\",\n",
    "    \"VOCAB\": \"ko_word2vec.model\",\n",
    "\n",
    "## Data params\n",
    "    \"INPUT_DIM\": 128,\n",
    "    \"N_CLASSES\": 2,\n",
    "    \"MAX_LENGTH\": 64,\n",
    "    \"PADDING\": False,\n",
    "\n",
    "## Model params\n",
    "    \"HIDDEN\": 128,\n",
    "    \"EMBEDDING_DIM\": 128, #RNN\n",
    "    \"N_LAYER\": 2, #LSTM\n",
    "    \"BIDIRECT\": True, #LSTM\n",
    "    \"DROPOUT\": 0.5,\n",
    "   \n",
    "\n",
    "## Training params\n",
    "    \"BATCH_FIRST\": True,\n",
    "    \"LR\": 0.0001,\n",
    "    \"WARMUP_RATIO\": 0.2,\n",
    "    \"MAX_EPOCHES\": 100,\n",
    "    \"BATCH_SZ\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab_path, name=\"Vocab\"):\n",
    "        self.name = name\n",
    "\n",
    "        model = Word2Vec.load(vocab_path)\n",
    "        vocab = [word for word in model.wv.index_to_key]\n",
    "        self.vocab = [\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[CLS]\", \"[UNK]\"] + vocab\n",
    "\n",
    "        self.word2index = dict([(word, i) for i, word in enumerate(self.vocab)])\n",
    "        self.index2word = dict([(i, word) for i, word in enumerate(self.vocab)])\n",
    "        self.n_words = len(self.vocab) # Count default tokens\n",
    "    def handle_unknown_word():\n",
    "        pass\n",
    "\n",
    "kor_vocab = Vocab(\"ko_word2vec.model\")\n",
    "# kor_sign_vocab.index2word[1]\n",
    "\n",
    "PAD_IDX = kor_vocab.word2index[\"[PAD]\"]\n",
    "EOS_IDX = kor_vocab.word2index[\"[EOS]\"]\n",
    "UNK_IDX = kor_vocab.word2index[\"[UNK]\"]\n",
    "\n",
    "base_params[\"N_VOCAB\"] = kor_vocab.n_words\n",
    "print(kor_vocab.n_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kobert import get_tokenizer\n",
    "# from kobert import get_pytorch_kobert_model\n",
    "\n",
    "# bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, max_len):\n",
    "\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=True, pair=False)\n",
    "\n",
    "        self.sentences = [transform([i[0]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[2]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "class BERTData_Module(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.data_dir = args[\"DATA\"]\n",
    "        self.batch_size = args[\"BATCH_SZ\"]\n",
    "        self.vocab_dir = args[\"VOCAB\"]\n",
    "        self.args = args\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        self.sentences = []\n",
    "        with open(self.data_dir, 'r') as f:\n",
    "            sentences = json.load(f)\n",
    "            for sentence in sentences:\n",
    "                if(len(sentence[1]) > 0):\n",
    "                    self.sentences.append(sentence)\n",
    "        # self.sentences = self.sentences[:len(self.sentences)//10]\n",
    "        self.sentences = random.sample(self.sentences, len(self.sentences)//5)\n",
    "        \n",
    "      \n",
    "        tokenizer = get_tokenizer()\n",
    "        self.tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "        print('Vocab size: ', len(vocab))\n",
    "        print(\"load %d samples\"%(len(self.sentences)))\n",
    "       \n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self._train, self._val = train_test_split(self.sentences, test_size=0.3)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self._test = self.sentences\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                BERTDataset(self._train, bert_tokenizer=self.tok, max_len=self.args[\"MAX_LENGTH\"]),\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers= 8,\n",
    "                shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                BERTDataset(self._val, bert_tokenizer=self.tok, max_len=self.args[\"MAX_LENGTH\"]),\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers= 8,\n",
    "                shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                BERTDataset(self._test, bert_tokenizer=self.tok, max_len=self.args[\"MAX_LENGTH\"]),\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers= 8,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = BERTData_Module(args=base_params)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "for batch in datamodule.train_dataloader():\n",
    "    # print(batch.size())\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "import itertools\n",
    "\n",
    "class EdgeEncoder(object):\n",
    "    def __init__(self, model_name='ko_word2vec.model', dtype=None):\n",
    "        self.dtype = dtype\n",
    "        self.model = Word2Vec.load(model_name)\n",
    "\n",
    "    def __call__(self, node1, node2):\n",
    "\n",
    "        try:\n",
    "            edge_weight = self.model.wv.similarity(node1, node2)\n",
    "            edge_weight = torch.tensor(edge_weight).to(self.dtype)\n",
    "        except:\n",
    "            edge_weight = torch.tensor(0.).to(self.dtype)\n",
    "        return edge_weight\n",
    "\n",
    "class SequenceEncoder(object):\n",
    "    def __init__(self, model_name='ko_word2vec.model', dtype=None, size=128):\n",
    "        \n",
    "        self.model = Word2Vec.load(model_name)\n",
    "        self.dtype = dtype\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, word):\n",
    "\n",
    "        try:\n",
    "            x = self.model.wv[word]\n",
    "            x = torch.from_numpy(x).view(self.size).to(self.dtype)\n",
    "        except:\n",
    "            x = torch.zeros(self.size).to(self.dtype)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NSMC_Graph(Dataset):\n",
    "    def __init__(self, sentences, args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sentences = sentences\n",
    "        \n",
    "        self.size = args[\"INPUT_DIM\"]\n",
    "        self.node_encoder = SequenceEncoder(args[\"MODEL_NAME\"], dtype=torch.float, size=self.size)\n",
    "        self.edge_encoder = EdgeEncoder(args[\"MODEL_NAME\"], dtype=torch.float)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def get(self, idx, return_sample=False):\n",
    "        data = self.sentences[idx]\n",
    "       \n",
    "        tokens = data[1]\n",
    "      \n",
    "        xs = [self.node_encoder(token) for token in tokens]\n",
    "        edge_index = list(itertools.product(range(len(tokens)), range(len(tokens))))\n",
    "        edge_attr = [self.edge_encoder(tokens[i], tokens[j]) for (i, j) in edge_index]\n",
    "       \n",
    "        edge_index = torch.tensor(edge_index)\n",
    "        edge_attr = torch.stack(edge_attr)\n",
    "        x = torch.stack(xs)\n",
    "        label = torch.tensor(data[2]).to(torch.long)\n",
    "        if(return_sample == True): \n",
    "            return Data(x=x, edge_index=edge_index.T, edge_weight=edge_attr, y=label), tokens\n",
    "        else:\n",
    "            return Data(x=x, edge_index=edge_index.T, edge_weight=edge_attr, y=label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"NLP_data\", \"nsmc_test.json\"), 'r') as f:\n",
    "    nscm_data = json.load(f)\n",
    "   \n",
    "\n",
    "dataset = NSMC_Graph(nscm_data[:200], base_params)\n",
    "data = dataset[5]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def draw_graph(g, labels, edge_mask=None, draw_edge_labels=False):\n",
    "    # g = g.copy().to_undirected()\n",
    "    g = to_networkx(g, edge_attrs=[\"edge_weight\"], to_undirected=True)\n",
    "    node_labels = {}\n",
    "    for i in range(len(labels)):\n",
    "        node_labels[i] = labels[i]\n",
    "        \n",
    "    # pos = nx.planar_layout(g)\n",
    "    # pos = nx.spring_layout(g, pos=pos)\n",
    "    if edge_mask is None:\n",
    "        edge_color = 'black'\n",
    "        widths = None\n",
    "    else:\n",
    "        edge_color = [data[\"edge_weight\"] for u, v, data in g.edges(data=True)]\n",
    "        widths = [x * 5 for x in edge_color]\n",
    "\n",
    "    # fontprop = fm.FontProperties(fname='NanumGothic.otf', size=18)\n",
    "    nx.draw(g, labels=node_labels, width=widths,\n",
    "            edge_color=edge_color, edge_cmap=plt.cm.Blues,\n",
    "            node_color='azure', font_family=\"NanumBarunGothic\")\n",
    "    \n",
    "    if draw_edge_labels and edge_mask is not None:\n",
    "        edge_labels = {k: ('%.2f' % v) for k, v in edge_mask.items()}    \n",
    "        nx.draw_networkx_edge_labels(g, edge_labels=edge_labels,\n",
    "                                    font_color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "i = random.choice(range(dataset.len()))\n",
    "data = dataset.get(i, return_sample=True)\n",
    "print(data)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "draw_graph(g = data[0], labels=data[1], edge_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "# from torch_geometric.loader import DataLoader\n",
    "import torch_geometric\n",
    "\n",
    "class GraphDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.data_dir = args[\"DATA\"]\n",
    "        self.batch_size = args[\"BATCH_SZ\"]\n",
    "        self.args = args\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        self.sentences = []\n",
    "        with open(self.data_dir, 'r') as f:\n",
    "            sentences = json.load(f)\n",
    "            for sentence in sentences:\n",
    "                if(len(sentence[1]) > 0):\n",
    "                    self.sentences.append(sentence)\n",
    "        # self.sentences = self.sentences[:len(self.sentences)//10]\n",
    "        self.sentences = random.sample(self.sentences, len(self.sentences)//5)\n",
    "        print(\"load %d samples\"%(len(self.sentences)))\n",
    "       \n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self._train, self._val = train_test_split(self.sentences, test_size=0.3)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self._test = self.sentences\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return torch_geometric.loader.DataLoader(\n",
    "                NSMC_Graph(self._train, args=self.args),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch_geometric.loader.DataLoader(\n",
    "                NSMC_Graph(self._val, args=self.args),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch_geometric.loader.DataLoader(\n",
    "                NSMC_Graph(self._test, args=self.args),\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = GraphDataModule(args=base_params)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "for batch in datamodule.train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class NSMC_seq(Dataset):\n",
    "    def __init__(self, data, vocab, args):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.padding = args[\"PADDING\"]\n",
    "        self.max_length = args[\"MAX_LENGTH\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        _, item, label = self.data[idx]\n",
    "        \n",
    "        # adding PAD, EOS, SOS and truncated\n",
    "        tokens = [self.vocab.word2index[\"[SOS]\"]] + [self.vocab.word2index[token] if(token in self.vocab.word2index) else self.vocab.word2index[\"[UNK]\"] \n",
    "                        for token in item][:self.max_length-2] + [self.vocab.word2index[\"[EOS]\"]]\n",
    "       \n",
    "        if(self.padding == True):\n",
    "            tokens += [self.vocab.word2index[\"[PAD]\"] for i in range(self.max_length - len(tokens))]\n",
    "          \n",
    "        return (tokens, label)\n",
    "\n",
    "def my_collate(batch):\n",
    "    max_len_input = max([len(item[0]) for item in batch])\n",
    "    label = [item[1] for item in batch]\n",
    "    len_input = [min(len(item[0]), max_len_input) for item in batch]\n",
    "\n",
    "    tokens = [item[0] + [PAD_IDX for i in range(max_len_input - len(item[0]))] for item in batch]\n",
    "   \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(tokens, dtype=int),\n",
    "        \"src_length\": torch.tensor(len_input, dtype=int),\n",
    "        \"labels\": torch.tensor(label, dtype=int)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "class SeqDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.data_dir = args[\"DATA\"]\n",
    "        self.batch_size = args[\"BATCH_SZ\"]\n",
    "        self.vocab_dir = args[\"VOCAB\"]\n",
    "        self.args = args\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        self.sentences = []\n",
    "        with open(self.data_dir, 'r') as f:\n",
    "            sentences = json.load(f)\n",
    "            for sentence in sentences:\n",
    "                if(len(sentence[1]) > 0):\n",
    "                    self.sentences.append(sentence)\n",
    "        # self.sentences = self.sentences[:len(self.sentences)//10]\n",
    "        self.sentences = random.sample(self.sentences, len(self.sentences)//5)\n",
    "        \n",
    "        self.vocab = Vocab(self.vocab_dir)\n",
    "        print('Vocab size: ', self.vocab.n_words)\n",
    "        print(\"load %d samples\"%(len(self.sentences)))\n",
    "       \n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self._train, self._val = train_test_split(self.sentences, test_size=0.3)\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == 'test' or stage is None:\n",
    "            self._test = self.sentences\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                NSMC_seq(self._train, self.vocab, args=self.args),\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers= 8,\n",
    "                shuffle=True,\n",
    "                collate_fn=my_collate)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                NSMC_seq(self._val, self.vocab, args=self.args),\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers= 8,\n",
    "                shuffle=False,\n",
    "                collate_fn=my_collate)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                NSMC_seq(self._test, self.vocab, args=self.args),\n",
    "                batch_size=self.batch_size,\n",
    "                num_workers= 8,\n",
    "                shuffle=False,\n",
    "                collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = SeqDataModule(args=base_params)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "for batch in datamodule.train_dataloader():\n",
    "    print(batch[\"input_ids\"].size())\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GraphConv, global_add_pool\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim, num_features, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(num_features, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        # self.conv3 = GraphConv(dim, dim)\n",
    "        # self.conv4 = GraphConv(dim, dim)\n",
    "        # self.conv5 = GraphConv(dim, dim)\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
    "        # x = self.conv3(x, edge_index, edge_weight).relu()\n",
    "        # x = self.conv4(x, edge_index, edge_weight).relu()\n",
    "        # x = self.conv5(x, edge_index, edge_weight).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        hidden = self.fc(hidden.squeeze(0))\n",
    "       \n",
    "        return F.log_softmax(hidden, dim=-1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        # lengths need to be on CPU!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        hidden = self.fc(hidden)\n",
    "\n",
    "            \n",
    "        return F.log_softmax(hidden, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        #pooled = [batch size, embedding_dim]\n",
    "        hidden = self.fc(pooled)\n",
    "                \n",
    "        return F.log_softmax(hidden, dim=-1)\n",
    "\n",
    "\n",
    "def initialize_weights_FAST(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "    w2v = Word2Vec.load('ko_word2vec.model')\n",
    "    pretrained_embeddings = torch.tensor(w2v.syn1neg)\n",
    "    pretrained_embeddings = torch.concat([torch.zeros(5, base_params[\"EMBEDDING_DIM\"]), pretrained_embeddings]) #extend special characters\n",
    "    m.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    # m.embedding.weight.data[UNK_IDX] = torch.zeros(base_params[\"EMBEDDING_DIM\"])\n",
    "    # m.embedding.weight.data[PAD_IDX] = torch.zeros(base_params[\"EMBEDDING_DIM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return  F.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Module(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, kwargs):\n",
    "        super(Module, self).__init__()\n",
    "\n",
    "        if(kwargs[\"NAME\"] == \"GNN\"):\n",
    "\n",
    "            self.model = Net(dim = kwargs[\"INPUT_DIM\"], num_features=kwargs[\"HIDDEN\"], num_classes=kwargs[\"N_CLASSES\"])\n",
    "            initialize_weights(self.model)\n",
    "\n",
    "        elif(kwargs[\"NAME\"] == \"RNN\"):\n",
    "\n",
    "            self.model = RNN(kwargs[\"N_VOCAB\"], kwargs[\"EMBEDDING_DIM\"], kwargs[\"HIDDEN\"], kwargs[\"N_CLASSES\"])\n",
    "            initialize_weights(self.model)\n",
    "\n",
    "        elif(kwargs[\"NAME\"] == \"LSTM\"):\n",
    "\n",
    "            self.model = LSTM(vocab_size=kwargs[\"N_VOCAB\"], embedding_dim=kwargs[\"EMBEDDING_DIM\"], hidden_dim=kwargs[\"HIDDEN\"], output_dim=kwargs[\"N_CLASSES\"],\n",
    "                                n_layers=kwargs[\"N_LAYER\"], bidirectional=kwargs[\"BIDIRECT\"], dropout=kwargs[\"DROPOUT\"], pad_idx=PAD_IDX)\n",
    "            initialize_weights(self.model)\n",
    "\n",
    "        elif(kwargs[\"NAME\"] == \"FAST\"):\n",
    "            self.model = FastText(vocab_size=kwargs[\"N_VOCAB\"], embedding_dim=kwargs[\"EMBEDDING_DIM\"], output_dim=kwargs[\"N_CLASSES\"], pad_idx=PAD_IDX)\n",
    "            initialize_weights_FAST(self.model)\n",
    "\n",
    "        elif(kwargs[\"NAME\"] == \"BERT\"):\n",
    "\n",
    "            self.model = BERTClassifier(bert=bertmodel, num_classes=kwargs[\"N_CLASSES\"], dr_rate=kwargs[\"DROPOUT\"])\n",
    "        \n",
    "        self.loss_function = torch.nn.NLLLoss()\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Prepare optimizer\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr= self.kwargs[\"LR\"])\n",
    "        # warm up lr\n",
    "        num_train_steps = len(self.trainer._data_connector._train_dataloader_source.dataloader()) * self.kwargs[\"MAX_EPOCHES\"] ##because of lighting problem from 1.5 version\n",
    "        num_warmup_steps = int(num_train_steps * self.kwargs[\"WARMUP_RATIO\"])\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "        lr_scheduler = {'scheduler': scheduler, 'name': 'cosine_schedule_with_warmup',\n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        if(self.kwargs[\"NAME\"] == \"GNN\"):\n",
    "            batch = batch.to(device)\n",
    "            output = self.model(x=batch.x, edge_index=batch.edge_index, batch=batch.batch, edge_weight=batch.edge_weight)\n",
    "           \n",
    "        elif(self.kwargs[\"NAME\"] in [\"RNN\", \"FAST\"]): \n",
    "            x = torch.swapaxes(batch[\"input_ids\"], 0, 1)\n",
    "            output = self.model(x)\n",
    "\n",
    "        elif(self.kwargs[\"NAME\"] == \"LSTM\"): \n",
    "            x = torch.swapaxes(batch[\"input_ids\"], 0, 1)\n",
    "            output = self.model(x, batch[\"src_length\"])\n",
    "        \n",
    "        elif(self.kwargs[\"NAME\"] == \"BERT\"):\n",
    "            token_ids, valid_length, segment_ids, _ =  batch\n",
    "            token_ids = token_ids.long()\n",
    "            segment_ids = segment_ids.long()\n",
    "            valid_length= valid_length\n",
    "            output = self.model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def cal_loss(self, outputs, trg):\n",
    "        \n",
    "        return self.loss_function(outputs, trg)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "      \n",
    "        outputs = self(batch)\n",
    "\n",
    "        if(self.kwargs[\"NAME\"] == \"GNN\"): target = batch.y\n",
    "        elif(self.kwargs[\"NAME\"] in [\"RNN\", \"LSTM\", \"FAST\"]): target = batch[\"labels\"]\n",
    "        elif(self.kwargs[\"NAME\"] == \"BERT\"): target = batch[3].long()\n",
    "           \n",
    "        loss = self.cal_loss(outputs, target)\n",
    "        \n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True, batch_size=self.kwargs[\"BATCH_SZ\"])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "             \n",
    "        outputs = self(batch)\n",
    "\n",
    "        if(self.kwargs[\"NAME\"] == \"GNN\"): target = batch.y\n",
    "        elif(self.kwargs[\"NAME\"] in [\"RNN\", \"LSTM\", \"FAST\"]): target = batch[\"labels\"]\n",
    "        elif(self.kwargs[\"NAME\"] == \"BERT\"): target = batch[3].long()\n",
    "           \n",
    "        loss = self.cal_loss(outputs, target)\n",
    "        self.log('val_loss', loss, batch_size=self.kwargs[\"BATCH_SZ\"])\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "      \n",
    "        outputs = self(batch)\n",
    "\n",
    "        if(self.kwargs[\"NAME\"] == \"GNN\"): target = batch.y\n",
    "        elif(self.kwargs[\"NAME\"] in [\"RNN\", \"LSTM\", \"FAST\"]): target = batch[\"labels\"]\n",
    "        elif(self.kwargs[\"NAME\"] == \"BERT\"): target = batch[3].long()\n",
    "           \n",
    "        loss = self.cal_loss(outputs, target)\n",
    "        self.log('test_loss', loss, batch_size=self.kwargs[\"BATCH_SZ\"])\n",
    "        \n",
    "        _, outs = torch.max(outputs, dim=-1)\n",
    "        return {'output': outs, 'label': target} #be careful, it gathers data on each GPUs so the data will be split\n",
    "\n",
    "    def test_epoch_end(self, test_step_outputs):\n",
    "        acc = []\n",
    "        \n",
    "        for out in test_step_outputs:\n",
    "\n",
    "            labels = out[\"label\"]\n",
    "            outputs = out[\"output\"]\n",
    "            \n",
    "            acc += [outputs.eq(labels).sum().item() / self.kwargs[\"BATCH_SZ\"]]\n",
    "        print(\"Accuracy: \", (np.array(acc)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module = BERTData_Module(args=base_params)\n",
    "# data_module.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Module(kwargs=base_params)\n",
    "# loss = torch.nn.NLLLoss()\n",
    "# batch = next(iter(data_module.train_dataloader()))\n",
    "\n",
    "# outputs = model(batch)\n",
    "\n",
    "# print(outputs.type())\n",
    "# print(outputs.size())\n",
    "# l = loss(outputs, batch[3].long())\n",
    "# print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "data_module = GraphDataModule(args=base_params)\n",
    "# data_module = BERTData_Module(args=base_params)\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # dirpath='model_chp',\n",
    "    filename='{epoch:02d}',\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model = Module(kwargs=base_params)\n",
    "trainer = Trainer(\n",
    "    callbacks=checkpoint_callback,\n",
    "    max_epochs=base_params[\"MAX_EPOCHES\"],\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    strategy=\"dp\",\n",
    "    )\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "print('best model path {}'.format(checkpoint_callback.best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import SE\n",
    "# from pytorch_lightning import Trainer\n",
    "\n",
    "# data_module = BERTData_Module(args=base_params)\n",
    "# data_module.setup(stage=\"test\")\n",
    "# model = Module(kwargs=base_params)\n",
    "# trainer = Trainer(accelerator=\"gpu\", strategy=\"dp\", devices=1)\n",
    "# trainer.test(model=model, datamodule=data_module, ckpt_path=\"lightning_logs/BERT/checkpoints/epoch=01.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('GNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e215ceb6bc66c02176aac9a16ef7bdd77eb4405484a4a652fefcacf52fac4f29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
